{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans found: 4000 , Total Headers 4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>00011542_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>./data/training/00011542_000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>00010684_008.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>./data/training/00010684_008.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>00009595_014.png</td>\n",
       "      <td>Effusion</td>\n",
       "      <td>./data/training/00009595_014.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image Index Finding Labels                              path\n",
       "812   00011542_000.png     No Finding  ./data/training/00011542_000.png\n",
       "987   00010684_008.png     No Finding  ./data/training/00010684_008.png\n",
       "3632  00009595_014.png       Effusion  ./data/training/00009595_014.png"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_xray_df = pd.read_csv('./data/training_labels.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join( './data/training', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['Label'] = all_xray_df['Finding Labels'].map(lambda x: 0 if x=='No Finding' else 1.0)\n",
    "#all_xray_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_xray_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "out_path = './data/agument/'\n",
    "\n",
    "abnormal =  all_xray_df [ all_xray_df['Label'] == 1.0 ] \n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "  os.mkdir(out_path)\n",
    "\n",
    "  for idx,row in abnormal.iterrows():\n",
    "      img = cv2.imread(row['path'],cv2.IMREAD_COLOR)\n",
    "      fimg = cv2.flip(img,1)\n",
    "      cv2.imwrite(out_path+'F'+os.path.basename(row['path']),fimg)\n",
    "\n",
    "column_name = ['Image Index','Finding Labels','path','Label']\n",
    "data_list = []\n",
    "flist = glob.glob(out_path+'*.png')\n",
    "for f in flist:\n",
    "    path_name = f\n",
    "    base_name = os.path.basename(f)\n",
    "    data_list.append([base_name,'Effusion',path_name,1.0])\n",
    "\n",
    "flip_df = pd.DataFrame(columns=column_name,data=data_list)\n",
    "\n",
    "\n",
    "all_xray_df = pd.concat([all_xray_df,flip_df])\n",
    "all_xray_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils  import shuffle\n",
    "\n",
    "\n",
    "\n",
    "normal =  all_xray_df [ all_xray_df['Label'] == 0.0 ]\n",
    "abnormal =  all_xray_df [ all_xray_df['Label'] == 1.0 ]\n",
    "\n",
    "X_train = pd.concat([normal.sample(frac=0.8, random_state=0),\\\n",
    "                     abnormal.sample(frac=0.8,random_state=0)], axis=0)\n",
    "X_valid = all_xray_df.loc[~all_xray_df.index.isin(X_train.index)]\n",
    "\n",
    "X_train = shuffle(X_train)\n",
    "X_valid = shuffle(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train['path'].values\n",
    "train_y = X_train['Label'].values\n",
    "\n",
    "train_image = []\n",
    "for f in train_x:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    \n",
    "    train_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "train_label = np.column_stack([1-train_y,train_y])\n",
    "\n",
    "\n",
    "valid_x = X_valid['path'].values\n",
    "valid_y = X_valid['Label'].values\n",
    "\n",
    "valid_image = []\n",
    "for f in valid_x:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    valid_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "valid_label = np.column_stack([1-valid_y,valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3840, 960)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images = tf.placeholder(tf.float32, [None, 64, 64, 1])\n",
    "true_out = tf.placeholder(tf.float32, [ None, 2])\n",
    "kprob = tf.placeholder_with_default(0.9,shape=())\n",
    "\n",
    "\n",
    "xavi_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "w1 = tf.Variable(xavi_init(shape=[3,3,1,32]),name='conv1_w')\n",
    "b1 = tf.Variable(tf.zeros([32]) ,name='conv1_b')\n",
    "\n",
    "conv1 = tf.nn.conv2d(images, w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "bias1 = tf.nn.bias_add(conv1, b1)\n",
    "relu1 = tf.nn.relu(bias1)\n",
    "relu1 = tf.nn.dropout(relu1,kprob)\n",
    "\n",
    "\n",
    "w2 = tf.Variable(xavi_init(shape=[3,3,32,32]),name='conv2_w')\n",
    "b2 = tf.Variable(tf.zeros([32]) ,name='conv2_b')\n",
    "\n",
    "conv2 = tf.nn.conv2d(relu1, w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "bias2 = tf.nn.bias_add(conv2, b2)\n",
    "relu2 = tf.nn.relu(bias2)\n",
    "relu2 = tf.nn.dropout(relu2,kprob)\n",
    "\n",
    "# 32, 32\n",
    "pool1 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "\n",
    "w3 = tf.Variable(xavi_init(shape=[3,3,32,64]),name='conv3_w')\n",
    "b3 = tf.Variable(tf.zeros([64]) ,name='conv3_b')\n",
    "\n",
    "conv3 = tf.nn.conv2d(pool1, w3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "bias3 = tf.nn.bias_add(conv3, b3)\n",
    "relu3 = tf.nn.relu(bias3)\n",
    "relu3 = tf.nn.dropout(relu3,kprob)\n",
    "\n",
    "\n",
    "w4 = tf.Variable(xavi_init(shape=[3,3,64,64]),name='conv4_w')\n",
    "b4 = tf.Variable(tf.zeros([64]) ,name='conv4_b')\n",
    "\n",
    "conv4 = tf.nn.conv2d(relu3, w4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "bias4 = tf.nn.bias_add(conv4, b4)\n",
    "relu4 = tf.nn.relu(bias4)\n",
    "relu4 = tf.nn.dropout(relu4,kprob)\n",
    "\n",
    "w5 = tf.Variable(xavi_init(shape=[3,3,64,64]),name='conv5_w')\n",
    "b5 = tf.Variable(tf.zeros([64]) ,name='conv5_b')\n",
    "\n",
    "conv5 = tf.nn.conv2d(relu4, w5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "bias5 = tf.nn.bias_add(conv5, b5)\n",
    "relu5 = tf.nn.relu(bias5)\n",
    "relu5 = tf.nn.dropout(relu5,kprob)\n",
    "\n",
    "\n",
    "# 16, 16 , 64\n",
    "pool2 = tf.nn.max_pool(relu5,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "fc1_reshape = tf.reshape(pool2, [-1, 16384])\n",
    "fc1_w = tf.Variable(xavi_init(shape=[16384,1024]),name='fc1_w')\n",
    "fc1_b = tf.Variable(tf.zeros(shape=[1024]),name='fc1_b')\n",
    "fc1 = tf.nn.relu( tf.nn.bias_add( tf.matmul(fc1_reshape,fc1_w) , fc1_b) )\n",
    "fc1 = tf.nn.dropout(fc1,kprob)\n",
    "\n",
    "\n",
    "\n",
    "fc2_w = tf.Variable(xavi_init(shape=[1024,128]),name='fc2_w')\n",
    "fc2_b = tf.Variable(tf.zeros(shape=[128]),name='fc2_b')\n",
    "fc2 = tf.nn.relu( tf.nn.bias_add( tf.matmul(fc1,fc2_w) , fc2_b) )\n",
    "fc2 = tf.nn.dropout(fc2,kprob)\n",
    "\n",
    "fc3_w = tf.Variable(xavi_init(shape=[128,2]),name='fc3_w')\n",
    "fc3_b = tf.Variable(tf.zeros(shape=[2]),name='fc3_b')\n",
    "logit = tf.nn.bias_add( tf.matmul(fc2,fc3_w) , fc3_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost function , accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=true_out,logits=logit)\n",
    "total_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(total_loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logit, 1),tf.argmax(true_out, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.55787516 0.55\n",
      "VALID ----- EPOCH : 0, LOSS : 5.578751564025879, ACCR : --\n",
      "RECALL :0.3687499998847656 , PRECISION : 0.6145833330132379 , F1 SCORE :0.4609374529449511\n",
      "960.0 684.0\n",
      "1 0.48833308 0.725\n",
      "VALID ----- EPOCH : 1, LOSS : 4.883330762386322, ACCR : --\n",
      "RECALL :0.6937499997832031 , PRECISION : 0.7070063692015903 , F1 SCORE :0.7003154071968112\n",
      "960.0 770.0\n",
      "2 0.45540628 0.8\n",
      "VALID ----- EPOCH : 2, LOSS : 4.554062783718109, ACCR : --\n",
      "RECALL :0.7687499997597655 , PRECISION : 0.6473684208822714 , F1 SCORE :0.7028570930236769\n",
      "960.0 752.0\n",
      "3 0.40815917 0.825\n",
      "VALID ----- EPOCH : 3, LOSS : 4.0815916657447815, ACCR : --\n",
      "RECALL :0.7406249997685547 , PRECISION : 0.6970588233243944 , F1 SCORE :0.7181817680101045\n",
      "960.0 774.0\n",
      "4 0.38542658 0.8\n",
      "VALID ----- EPOCH : 4, LOSS : 3.854265809059143, ACCR : --\n",
      "RECALL :0.771874999758789 , PRECISION : 0.663978494445167 , F1 SCORE :0.7138727824459588\n",
      "960.0 762.0\n",
      "5 0.41598377 0.825\n",
      "VALID ----- EPOCH : 5, LOSS : 4.1598376631736755, ACCR : --\n",
      "RECALL :0.7281249997724609 , PRECISION : 0.7103658534419616 , F1 SCORE :0.7191357522548045\n",
      "960.0 778.0\n",
      "6 0.3411739 0.9\n",
      "VALID ----- EPOCH : 6, LOSS : 3.411738872528076, ACCR : --\n",
      "RECALL :0.7156249997763672 , PRECISION : 0.786941580485587 , F1 SCORE :0.7495907845644934\n",
      "960.0 807.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7a6c8d3fadfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlabel_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#batch_size = 500\n",
    "batch_size = 100\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for epoch in range(100):\n",
    "\n",
    "        for start in range(0,len(train_image),batch_size):\n",
    "\n",
    "            end = start + batch_size  if start + batch_size  < len(train_image)  else  len(train_image)\n",
    "\n",
    "            image_iter = train_image[start:end]\n",
    "            label_iter = train_label[start:end]\n",
    "\n",
    "            sess.run(train_op, feed_dict={images: image_iter, true_out:label_iter})\n",
    "\n",
    "            if end == len(train_image) :\n",
    "                loss,accr,train_logit = sess.run([total_loss,accuracy,logit], feed_dict={images: image_iter, true_out:label_iter})\n",
    "                print(epoch,loss,accr)\n",
    "\n",
    "                \n",
    "        \n",
    "        tp= fn= fp= tn= vloss = 0.0\n",
    "        \n",
    "        for start in range(0,len(valid_image),batch_size):\n",
    "\n",
    "            end = start + batch_size  if start + batch_size  < len(valid_image)  else  len(valid_image)\n",
    "\n",
    "            image_iter = valid_image[start:end]\n",
    "            label_iter = valid_label[start:end]\n",
    "            \n",
    "            val_loss,val_logit = sess.run([total_loss,logit], feed_dict={images: image_iter, true_out:label_iter,kprob:1.0})\n",
    "            \n",
    "            \n",
    "            \n",
    "            y_true = np.argmax(label_iter,axis=1)\n",
    "            y_pred = np.argmax(val_logit,axis=1) \n",
    "\n",
    "            true_index =np.where(y_true == 1)\n",
    "            flase_index = np.where(y_true == 0)\n",
    "\n",
    "\n",
    "            tp += np.sum(y_pred[true_index])\n",
    "            fn += np.sum(y_pred[true_index] == 0)\n",
    "            fp += np.sum(y_pred[flase_index])\n",
    "            tn += np.sum(y_pred[flase_index] == 0)\n",
    "            \n",
    "            vloss += loss\n",
    "\n",
    "        recall = tp/(tp+fn+1e-7)\n",
    "        precision = tp/(tp+fp+1e-7)\n",
    "        f1score = 2*(recall*precision)/(recall+precision+1e-7)\n",
    "\n",
    "\n",
    "        print('VALID ----- EPOCH : {}, LOSS : {}, ACCR : --'.format(epoch,vloss))\n",
    "        print('RECALL :{} , PRECISION : {} , F1 SCORE :{}'.format(recall, precision, f1score ))\n",
    "        print(tp+fn+fp+tn, tp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
