{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df = pd.read_csv('./data/training_labels.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join( './data/training', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "#all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['Label'] = all_xray_df['Finding Labels'].map(lambda x: 0 if x=='No Finding' else 1.0)\n",
    "all_xray_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils  import shuffle\n",
    "\n",
    "\n",
    "\n",
    "normal =  all_xray_df [ all_xray_df['Label'] == 0.0 ]\n",
    "abnormal =  all_xray_df [ all_xray_df['Label'] == 1.0 ]\n",
    "\n",
    "X_train = pd.concat([normal.sample(frac=0.8, random_state=0),\\\n",
    "                     abnormal.sample(frac=0.8,random_state=0)], axis=0)\n",
    "X_valid = all_xray_df.loc[~all_xray_df.index.isin(X_train.index)]\n",
    "\n",
    "X_train = shuffle(X_train)\n",
    "X_valid = shuffle(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data agumentation\n",
    "- flip\n",
    "- random shfit\n",
    "- random affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "out_path = './data/agument/'\n",
    "\n",
    "abnormal_df =  X_train [ X_train['Label'] == 1.0 ] \n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "    for idx,row in abnormal_df.iterrows():\n",
    "        img = cv2.imread(row['path'],cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # flip     \n",
    "        transform_img = cv2.flip(img,1)\n",
    "        cv2.imwrite(out_path+'F'+os.path.basename(row['path']),transform_img)\n",
    "\n",
    "        \"\"\"\n",
    "        # random shift\n",
    "        rows,cols = img.shape\n",
    "        shift_var = np.random.randint(low=-6,high=6,size=2)\n",
    "        M = np.float32([[1,0,shift_var[0]],[0,1,shift_var[1]]])\n",
    "        transform_img = cv2.warpAffine(img,M,(cols,rows))\n",
    "        cv2.imwrite(out_path+'S'+os.path.basename(row['path']),transform_img)\n",
    "\n",
    "        # random affine\n",
    "        pts1 = np.float32([[8,8],[58,58],[32,58]])\n",
    "        shift_var = np.random.randint(low=-1,high=1,size=6).astype(np.float32) \n",
    "        pts2 = np.reshape(pts1.flatten()+shift_var,(-1,2))\n",
    "        \n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        transform_img = cv2.warpAffine(img,M,(cols,rows))        \n",
    "        cv2.imwrite(out_path+'A'+os.path.basename(row['path']),transform_img)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "column_name = ['Image Index','Finding Labels','path','Label']\n",
    "data_list = []\n",
    "flist = glob.glob(out_path+'*.png')\n",
    "for f in flist:\n",
    "    path_name = f\n",
    "    base_name = os.path.basename(f)\n",
    "    data_list.append([base_name,'Effusion',path_name,1.0])\n",
    "\n",
    "flip_df = pd.DataFrame(columns=column_name,data=data_list)\n",
    "\n",
    "\n",
    "X_train = pd.concat([X_train,flip_df])\n",
    "X_train.reset_index(inplace=True)\n",
    "\n",
    "X_train = shuffle(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train['path'].values\n",
    "train_y = X_train['Label'].values\n",
    "\n",
    "train_image = []\n",
    "for f in train_x:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    \n",
    "    train_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "train_label = np.column_stack([1-train_y,train_y])\n",
    "\n",
    "\n",
    "valid_x = X_valid['path'].values\n",
    "valid_y = X_valid['Label'].values\n",
    "\n",
    "valid_image = []\n",
    "for f in valid_x:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    valid_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "valid_label = np.column_stack([1-valid_y,valid_y])\n",
    "len(train_x),len(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "xavi_init = tf.contrib.layers.xavier_initializer\n",
    "\n",
    "\n",
    "class resnet():\n",
    "    \n",
    "    #xavi_init = tf.contrib.layers.xavier_initializer\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def short_layer(self,input_data,is_train,name,ksize=3):\n",
    "        shapes = input_data.get_shape().as_list()\n",
    "        with tf.name_scope('name'):\n",
    "            norm1 = tf.layers.batch_normalization(input_data,training=is_train)\n",
    "            batn1 = tf.nn.relu(norm1)\n",
    "            \n",
    "            weit1 = tf.get_variable(name+'_w1',shape=[ksize,ksize,shapes[-1],shapes[-1]],initializer=xavi_init())\n",
    "            conv1 = tf.nn.conv2d(batn1,weit1,strides=[1,1,1,1],padding='SAME')\n",
    "            \n",
    "            \n",
    "            norm2 = tf.layers.batch_normalization(conv1,training=is_train)\n",
    "            batn2 = tf.nn.relu(norm2)\n",
    "            \n",
    "            weit2 = tf.get_variable(name+'_w2',shape=[ksize,ksize,shapes[-1],shapes[-1]],initializer=xavi_init())\n",
    "            conv2 = tf.nn.conv2d(batn2,weit2,strides=[1,1,1,1],padding='SAME')\n",
    "            \n",
    "        return input_data + conv2\n",
    "\n",
    "    def build_basic(self,data,train_mode=True,k_prob=0.8):\n",
    "        weit1 = tf.get_variable('conv1_w',shape=[5,5,1,16],initializer=xavi_init())\n",
    "        conv1 = tf.nn.conv2d(data,weit1,strides=[1,1,1,1],padding='SAME')\n",
    "        \n",
    "        norm1 = tf.layers.batch_normalization(conv1,training=train_mode)\n",
    "        relu1 = tf.nn.relu(norm1)\n",
    "        \n",
    "        weit2 = tf.get_variable('conv2_w',shape=[3,3,16,16],initializer=xavi_init())\n",
    "        conv2 = tf.nn.conv2d(relu1,weit2,strides=[1,1,1,1],padding='SAME')\n",
    "        \n",
    "        norm2 = tf.layers.batch_normalization(conv2,training=train_mode)\n",
    "        relu2 = tf.nn.relu(norm2)\n",
    "        \n",
    "        # 32, 32, 16\n",
    "        pool1 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        #pool1 = tf.nn.dropout(pool1,keep_prob=k_prob)\n",
    "        \n",
    "        weit3 = tf.get_variable('conv3_w',shape=[3,3,16,32],initializer=xavi_init())\n",
    "        conv3 = tf.nn.conv2d(pool1,weit3,strides=[1,1,1,1],padding='SAME')\n",
    "        \n",
    "        norm3 = tf.layers.batch_normalization(conv3,training=train_mode)\n",
    "        relu3 = tf.nn.relu(norm3)\n",
    " \n",
    "        weit4 = tf.get_variable('conv4_w',shape=[3,3,32,32],initializer=xavi_init())\n",
    "        conv4 = tf.nn.conv2d(relu3,weit4,strides=[1,1,1,1],padding='SAME')\n",
    "        \n",
    "        norm4 = tf.layers.batch_normalization(conv4,training=train_mode)\n",
    "        relu4 = tf.nn.relu(norm4)\n",
    "\n",
    "        # 16, 16, 32\n",
    "        pool2 = tf.nn.max_pool(relu4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        #pool2 = tf.nn.dropout(pool2,keep_prob=k_prob)\n",
    "        pool2 = tf.reshape(pool2, [-1, 16*16*32 ])     \n",
    "\n",
    "        \n",
    "        fc1_w = tf.get_variable('fc1_w',shape=[16*16*32,512],initializer=xavi_init())\n",
    "        fc1_b = tf.get_variable('fc1_b',shape=[512],initializer=tf.zeros_initializer() )         \n",
    "        fc1 = tf.add( tf.matmul(pool2,fc1_w) , fc1_b)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        fc1 = tf.nn.dropout(fc1,k_prob)\n",
    "        \n",
    "        fc2_w = tf.get_variable('fc2_w',shape=[512,64],initializer=xavi_init())\n",
    "        fc2_b = tf.get_variable('fc2_b',shape=[64],initializer=tf.zeros_initializer() )      \n",
    "        fc2 = tf.add( tf.matmul(fc1,fc2_w) , fc2_b)\n",
    "        fc2 = tf.nn.relu(fc2)        \n",
    "        fc2 = tf.nn.dropout(fc2,k_prob)\n",
    "        \n",
    "        fc3_w = tf.get_variable('fc3_w',shape=[64,2],initializer=xavi_init())\n",
    "        fc3_b = tf.get_variable('fc3_b',shape=[2],initializer=tf.zeros_initializer() )  \n",
    "        fc3 = tf.add( tf.matmul(fc2,fc3_w) , fc3_b)\n",
    "        \n",
    "        return fc3\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def build_simple(self,data,train_mode=True,k_prob=0.8):\n",
    "        conv1 = tf.layers.conv2d(data,32,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm1 = tf.layers.batch_normalization(conv1,training=train_mode)\n",
    "        relu1 = tf.nn.relu(norm1)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(relu1,32,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm2 = tf.layers.batch_normalization(conv2,training=train_mode)\n",
    "        relu2 = tf.nn.relu(norm2)\n",
    "\n",
    "        # 32, 32, 32\n",
    "        pool1 = tf.layers.max_pooling2d(relu2, pool_size=3, strides=2, padding='same')\n",
    "        pool1 = tf.nn.dropout(pool1,k_prob)\n",
    "            \n",
    "        conv3 = tf.layers.conv2d(pool1,64,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm3 = tf.layers.batch_normalization(conv3,training=train_mode)\n",
    "        relu3 = tf.nn.relu(norm3)\n",
    "\n",
    "        conv4 = tf.layers.conv2d(relu3,64,kernel_size=3,padding='same')\n",
    "        norm4 = tf.layers.batch_normalization(conv4,training=train_mode)\n",
    "        relu4 = tf.nn.relu(norm4)    \n",
    "        \n",
    "        \n",
    "        # 16, 16, 64\n",
    "        pool2 = tf.layers.max_pooling2d(relu4, pool_size=3, strides=2, padding='same')\n",
    "        #pool2 = tf.nn.dropout(pool2,k_prob)\n",
    "\n",
    "                \n",
    "        conv5 = tf.layers.conv2d(pool2,128,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm5 = tf.layers.batch_normalization(conv5,training=train_mode)\n",
    "        relu5 = tf.nn.relu(norm5)\n",
    "\n",
    "        conv6 = tf.layers.conv2d(relu5,128,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm6 = tf.layers.batch_normalization(conv6,training=train_mode)\n",
    "        relu6 = tf.nn.relu(norm6)            \n",
    "        \n",
    "        # 8, 8, 128\n",
    "        pool3 = tf.layers.max_pooling2d(relu6, pool_size=3, strides=2, padding='same')   \n",
    "        pool3 = tf.nn.dropout(pool3,k_prob)\n",
    "\n",
    "        \n",
    "        conv7 = tf.layers.conv2d(pool3,128,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm7 = tf.layers.batch_normalization(conv7,training=train_mode)\n",
    "        relu7 = tf.nn.relu(norm7)    \n",
    "        \n",
    "        conv8 = tf.layers.conv2d(relu7,128,kernel_size=3,padding='same',use_bias=False)\n",
    "        norm8 = tf.layers.batch_normalization(conv8,training=train_mode)\n",
    "        relu8 = tf.nn.relu(norm8)    \n",
    "        \n",
    "        \n",
    "        # 4, 4, 128\n",
    "        pool4 = tf.layers.max_pooling2d(relu8, pool_size=3, strides=2, padding='same')   \n",
    "        \n",
    "        net_shape = pool4.get_shape().as_list()\n",
    "        net = tf.reshape(pool4, [-1, net_shape[1] * net_shape[2] * net_shape[3]])     \n",
    "        \n",
    "        fc1 = tf.layers.dense(net,32,activation=tf.nn.relu)\n",
    "        fc1 = tf.nn.dropout(fc1,k_prob)\n",
    "        \n",
    "        fc2 = tf.layers.dense(fc1,2)\n",
    "        \n",
    "        return fc2\n",
    "    \n",
    "    def build_model(self,data,train_mode=True):\n",
    "        \n",
    "        # input 64,64,1\n",
    "        with tf.name_scope(\"conv_layer1\"):\n",
    "            \n",
    "            # conv1 - 5,5 64 ,stride\n",
    "            net = tf.layers.conv2d(data,32,kernel_size=5,activation=tf.nn.relu,padding='same')     \n",
    "            \n",
    "            # batch , 64,64,32\n",
    "            net = tf.layers.batch_normalization(net, training=train_mode)\n",
    "            \n",
    "            # Max pool 32,32,32\n",
    "            net = tf.layers.max_pooling2d(net, pool_size=3, strides=2, padding='same')\n",
    "            \n",
    "            \n",
    "        short1_1 = self.short_layer(net,train_mode,'short1_1')\n",
    "        short1_2 = self.short_layer(short1_1,train_mode,'short1_2')\n",
    "        short2_1 = self.short_layer(short1_2,train_mode,'short2_1')\n",
    "        short2_2 = self.short_layer(short2_1,train_mode,'short2_2')\n",
    "        \n",
    "        \n",
    "        # input 32,32,32\n",
    "        with tf.name_scope(\"conv_layer2\"):\n",
    "            \n",
    "            # conv1 - 3,3 64 ,stride\n",
    "            net = tf.layers.conv2d(short2_2,64,kernel_size=3,activation=tf.nn.relu,padding='same')     \n",
    "            \n",
    "            # batch , 32,32,64\n",
    "            net = tf.layers.batch_normalization(net, training=train_mode)        \n",
    "        \n",
    "            # Max pool 16,16,64\n",
    "            net = tf.layers.max_pooling2d(net, pool_size=3, strides=2, padding='same')        \n",
    "        \n",
    "        short3 = self.short_layer(net,train_mode,'short3')\n",
    "        short4 = self.short_layer(short3,train_mode,'short4')     \n",
    "        \n",
    "        \n",
    "        # input 16,16,64\n",
    "        with tf.name_scope(\"conv_layer2\"):\n",
    "            \n",
    "            # conv1 - 3,3 128 ,stride\n",
    "            net = tf.layers.conv2d(short4,128,kernel_size=3,activation=tf.nn.relu,padding='same')     \n",
    "            \n",
    "            # batch , 16,16,128\n",
    "            net = tf.layers.batch_normalization(net, training=train_mode)        \n",
    "        \n",
    "            # Max pool 8,8,128\n",
    "            net = tf.layers.max_pooling2d(net, pool_size=3, strides=2, padding='same')        \n",
    "        \n",
    "        short5 = self.short_layer(net,train_mode,'short5')\n",
    "        short6 = self.short_layer(short5,train_mode,'short6')     \n",
    "        \n",
    "        \n",
    "        # input 8,8,128\n",
    "        with tf.name_scope(\"conv_layer2\"):\n",
    "            \n",
    "            # conv1 - 3,3 256 ,stride\n",
    "            net = tf.layers.conv2d(short6,256,kernel_size=3,activation=tf.nn.relu,padding='same')     \n",
    "            \n",
    "            # batch , 8,8,256\n",
    "            net = tf.layers.batch_normalization(net, training=train_mode)        \n",
    "        \n",
    "            # Max pool 4,4,256\n",
    "            net = tf.layers.max_pooling2d(net, pool_size=3, strides=2, padding='same')        \n",
    "        \n",
    "        short7 = self.short_layer(net,train_mode,'short7')\n",
    "        short8 = self.short_layer(short7,train_mode,'short8')\n",
    " \n",
    "        short9 = self.short_layer(short8,train_mode,'short9')\n",
    "        short10 = self.short_layer(short7,train_mode,'short10')\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.global_logit(short10,2)\n",
    "\n",
    "        \n",
    "        \n",
    "    def global_logit(self,input_data,output):\n",
    "        shapes = input_data.get_shape().as_list()\n",
    "        \n",
    "        net = tf.nn.avg_pool(input_data,ksize=[1, shapes[1], shapes[2], 1],\\\n",
    "                       strides=[1, 1, 1, 1],padding='VALID')\n",
    "        \n",
    "        net_shape = net.get_shape().as_list()\n",
    "        net = tf.reshape(net, [-1, net_shape[1] * net_shape[2] * net_shape[3]])     \n",
    "        \n",
    "        net = tf.layers.dense(net,64, activation=None)\n",
    "        logits = tf.layers.dense(net,output, activation=None)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def model_loss(self,logits,labels):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=labels)\n",
    "        cross_mean = tf.reduce_mean(cross_entropy)\n",
    "        return cross_mean\n",
    "        \n",
    "    def model_train(self,loss,lr=0.005):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        return train_op\n",
    "    \n",
    "    def model_accuracy(self,logits,labels):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1),tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet()\n",
    "\n",
    "\n",
    "images = tf.placeholder(tf.float32, [None, 64, 64, 1])\n",
    "true_out = tf.placeholder(tf.float32, [ None, 2])\n",
    "\n",
    "kprob = tf.placeholder_with_default(0.8,shape=())\n",
    "train_mode = tf.placeholder_with_default(True,shape=())\n",
    "\n",
    "\n",
    "# resnet\n",
    "#logits = model.build_model(images,train_mode)\n",
    "\n",
    "# vgg\n",
    "#logits = model.build_simple(images,train_mode)\n",
    "\n",
    "# cnn\n",
    "logits = model.build_basic(images,train_mode,kprob)\n",
    "\n",
    "total_loss = model.model_loss(logits,true_out)\n",
    "train_op = model.model_train(total_loss,0.003)\n",
    "accuracy = model.model_accuracy(logits,true_out)\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "CHECKPOINT_PATH = './ckpt_basics/vgg_xray.ckpt'\n",
    "CHECKPOINT_FILE = '%s.meta' % CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 500\n",
    "batch_size = 200\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "max_f1score =0.0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if not os.path.isfile(CHECKPOINT_FILE):\n",
    "        sess.run(init_op)\n",
    "    \n",
    "\n",
    "        for epoch in range(350):\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for start in range(0,len(train_image),batch_size):\n",
    "\n",
    "                end = min( start+batch_size ,len(train_image))\n",
    "                image_iter = train_image[start:end]\n",
    "                label_iter = train_label[start:end]\n",
    "\n",
    "                _,train_loss = sess.run([train_op,total_loss], feed_dict={images: image_iter, true_out:label_iter})\n",
    "                epoch_loss += train_loss\n",
    "\n",
    "                if end == len(train_image) :\n",
    "                    #loss,accr = sess.run([accuracy], feed_dict={images: image_iter,true_out:label_iter})\n",
    "                    print(epoch,epoch_loss)\n",
    "\n",
    "\n",
    "\n",
    "            tp= fn= fp= tn= vloss = 0.0   \n",
    "            for start in range(0,len(valid_image),batch_size):\n",
    "\n",
    "                end = min( start+batch_size ,len(valid_image))\n",
    "\n",
    "                image_iter = valid_image[start:end]\n",
    "                label_iter = valid_label[start:end]\n",
    "\n",
    "                val_loss,val_logit = sess.run([total_loss,logits], feed_dict={images: image_iter, true_out:label_iter,kprob:1.0,train_mode:False})\n",
    "\n",
    "\n",
    "\n",
    "                y_true = np.argmax(label_iter,axis=1)\n",
    "                y_pred = np.argmax(val_logit,axis=1) \n",
    "\n",
    "                true_index =np.where(y_true == 1)\n",
    "                flase_index = np.where(y_true == 0)\n",
    "\n",
    "\n",
    "                tp += np.sum(y_pred[true_index])\n",
    "                fn += np.sum(y_pred[true_index] == 0)\n",
    "                fp += np.sum(y_pred[flase_index])\n",
    "                tn += np.sum(y_pred[flase_index] == 0)\n",
    "\n",
    "                vloss += val_loss\n",
    "\n",
    "            recall = tp/(tp+fn+1e-7)\n",
    "            precision = tp/(tp+fp+1e-7)\n",
    "            f1score = 2*(recall*precision)/(recall+precision+1e-7)\n",
    "\n",
    "\n",
    "            print('VALID ----- EPOCH : {:.3f}, LOSS : {:.3f}'.format(epoch,vloss))\n",
    "            print('RECALL :{:.3f} , PRECISION : {:.3f} , F1 SCORE :{:.3f}'.format(recall, precision, f1score ))\n",
    "            print('TP={}, TN={}, FP={}, FN={}'.format(tp,tn,fp,fn))\n",
    "\n",
    "            if(epoch > 60 and f1score > 0.67 ):\n",
    "              print('finded')\n",
    "              saver.save(sess, CHECKPOINT_PATH)  \n",
    "              break\n",
    "\n",
    "                  \n",
    "            \n",
    "    #saver.save(sess, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = glob.glob('./data/test/*.png')\n",
    "test_image = []\n",
    "for f in X_test:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    \n",
    "    test_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "print('test set' , np.shape(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_sizebatch_s  = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(CHECKPOINT_FILE):\n",
    "        saver.restore(sess, CHECKPOINT_PATH)\n",
    "        print('Restoring values from %s...' % CHECKPOINT_PATH)\n",
    "               \n",
    "\n",
    "\n",
    "        tp= fn= fp= tn= vloss = 0.0   \n",
    "        \n",
    "        \n",
    "        logit_list = []\n",
    "        \n",
    "        for start in range(0,len(test_image),batch_size):\n",
    "\n",
    "            end = min( start+batch_size ,len(test_image))\n",
    "\n",
    "            image_iter = test_image[start:end]\n",
    "\n",
    "            val_logit = sess.run(logits, feed_dict={images: image_iter,kprob:1.0,train_mode:False})\n",
    "            logit_list.append(val_logit)\n",
    "            \n",
    "        test_pred = np.argmax(np.reshape(logit_list,(-1,2)),axis=1)\n",
    "        test_df = pd.DataFrame({'PATH':X_test,'LABEL':test_pred})\n",
    "        test_df.to_csv('test_pred_vgg.csv',index=False)\n",
    "        print('------- DONE ------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
