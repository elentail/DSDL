{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df = pd.read_csv('./data/training_labels.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join( './data/training', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "#all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['Label'] = all_xray_df['Finding Labels'].map(lambda x: 0 if x=='No Finding' else 1.0)\n",
    "all_xray_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils  import shuffle\n",
    "\n",
    "\n",
    "normal =  all_xray_df [ all_xray_df['Label'] == 0.0 ]\n",
    "abnormal =  all_xray_df [ all_xray_df['Label'] == 1.0 ]\n",
    "\n",
    "X_train = pd.concat([normal.sample(frac=0.8, random_state=0),\\\n",
    "                     abnormal.sample(frac=0.8,random_state=0)], axis=0)\n",
    "X_valid = all_xray_df.loc[~all_xray_df.index.isin(X_train.index)]\n",
    "\n",
    "X_train = shuffle(X_train)\n",
    "X_valid = shuffle(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data agumentation with train set\n",
    "- flip\n",
    "- random shift\n",
    "- random affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "out_path = './data/agument/'\n",
    "\n",
    "abnormal_df =  X_train [ X_train['Label'] == 1.0 ] \n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "    for idx,row in abnormal_df.iterrows():\n",
    "        img = cv2.imread(row['path'],cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # flip     \n",
    "        transform_img = cv2.flip(img,1)\n",
    "        cv2.imwrite(out_path+'F'+os.path.basename(row['path']),transform_img)\n",
    "\n",
    "        # random shift\n",
    "        rows,cols = img.shape\n",
    "        shift_var = np.random.randint(low=-6,high=6,size=2)\n",
    "        M = np.float32([[1,0,shift_var[0]],[0,1,shift_var[1]]])\n",
    "        transform_img = cv2.warpAffine(img,M,(cols,rows))\n",
    "        cv2.imwrite(out_path+'S'+os.path.basename(row['path']),transform_img)\n",
    "\n",
    "        # random affine\n",
    "        pts1 = np.float32([[8,8],[58,58],[32,58]])\n",
    "        shift_var = np.random.randint(low=-1,high=1,size=6).astype(np.float32) \n",
    "        pts2 = np.reshape(pts1.flatten()+shift_var,(-1,2))\n",
    "        \n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        transform_img = cv2.warpAffine(img,M,(cols,rows))        \n",
    "        cv2.imwrite(out_path+'A'+os.path.basename(row['path']),transform_img)\n",
    "\n",
    "\n",
    "    \n",
    "column_name = ['Image Index','Finding Labels','path','Label']\n",
    "data_list = []\n",
    "flist = glob.glob(out_path+'*.png')\n",
    "for f in flist:\n",
    "    path_name = f\n",
    "    base_name = os.path.basename(f)\n",
    "    data_list.append([base_name,'Effusion',path_name,1.0])\n",
    "\n",
    "flip_df = pd.DataFrame(columns=column_name,data=data_list)\n",
    "\n",
    "\n",
    "X_train = pd.concat([X_train,flip_df])\n",
    "X_train.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('Label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over sample\n",
    "# train_over = pd.concat([X_train, X_train[X_train['Label'] == 1].sample(300) ])\n",
    "# train_over = shuffle(train_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#train_x = train_over['path'].values\n",
    "#train_y = train_over['Label'].values\n",
    "\n",
    "train_x = X_train['path'].values\n",
    "train_y = X_train['Label'].values\n",
    "\n",
    "train_image = []\n",
    "for f in train_x:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    \n",
    "    train_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "train_label = np.column_stack([1-train_y,train_y])\n",
    "\n",
    "\n",
    "valid_x = X_valid['path'].values\n",
    "valid_y = X_valid['Label'].values\n",
    "\n",
    "valid_image = []\n",
    "for f in valid_x:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    valid_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "valid_label = np.column_stack([1-valid_y,valid_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x),len(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "images = tf.placeholder(tf.float32, [None, 64, 64, 1])\n",
    "true_out = tf.placeholder(tf.float32, [ None, 2])\n",
    "kprob = tf.placeholder_with_default(0.9,shape=())\n",
    "train_mode = tf.placeholder_with_default(True,shape=())\n",
    "\n",
    "xavi_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "w1 = tf.Variable(xavi_init(shape=[3,3,1,32]),name='conv1_w')\n",
    "conv1 = tf.nn.conv2d(images, w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "norm1 = tf.layers.batch_normalization(conv1,training=train_mode)\n",
    "relu1 = tf.nn.relu(norm1)\n",
    "relu1 = tf.nn.dropout(relu1,kprob)\n",
    "\n",
    "\n",
    "w2 = tf.Variable(xavi_init(shape=[3,3,32,32]),name='conv2_w')\n",
    "conv2 = tf.nn.conv2d(relu1, w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "norm2 = tf.layers.batch_normalization(conv2,training=train_mode)\n",
    "relu2 = tf.nn.relu(norm2)\n",
    "relu2 = tf.nn.dropout(relu2,kprob)\n",
    "\n",
    "# 32, 32\n",
    "pool1 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "\n",
    "w3 = tf.Variable(xavi_init(shape=[3,3,32,64]),name='conv3_w')\n",
    "conv3 = tf.nn.conv2d(pool1, w3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "norm3 = tf.layers.batch_normalization(conv3,training=train_mode)\n",
    "relu3 = tf.nn.relu(norm3)\n",
    "relu3 = tf.nn.dropout(relu3,kprob)\n",
    "\n",
    "\n",
    "w4 = tf.Variable(xavi_init(shape=[3,3,64,64]),name='conv4_w')\n",
    "conv4 = tf.nn.conv2d(relu3, w4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "norm4 = tf.layers.batch_normalization(conv4,training=train_mode)\n",
    "relu4 = tf.nn.relu(norm4)\n",
    "relu4 = tf.nn.dropout(relu4,kprob)\n",
    "\n",
    "\n",
    "w5 = tf.Variable(xavi_init(shape=[3,3,64,64]),name='conv5_w')\n",
    "conv5 = tf.nn.conv2d(relu4, w5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "norm5 = tf.layers.batch_normalization(conv5,training=train_mode)\n",
    "relu5 = tf.nn.relu(norm5)\n",
    "relu5 = tf.nn.dropout(relu5,kprob)\n",
    "\n",
    "\n",
    "\n",
    "# 16, 16 , 64\n",
    "pool2 = tf.nn.max_pool(relu5,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "fc1_reshape = tf.reshape(pool2, [-1, 16384])\n",
    "fc1_w = tf.Variable(xavi_init(shape=[16384,1024]),name='fc1_w')\n",
    "fc1_n = tf.layers.batch_normalization(tf.matmul(fc1_reshape,fc1_w) ,training=train_mode)\n",
    "fc1 = tf.nn.relu(fc1_n)\n",
    "fc1 = tf.nn.dropout(fc1,kprob)\n",
    "\n",
    "\n",
    "\n",
    "fc2_w = tf.Variable(xavi_init(shape=[1024,128]),name='fc2_w')\n",
    "fc1_n = tf.layers.batch_normalization(tf.matmul(fc1,fc2_w) ,training=train_mode)\n",
    "fc2 = tf.nn.relu(fc1_n)\n",
    "fc2 = tf.nn.dropout(fc2,kprob)\n",
    "\n",
    "fc3_w = tf.Variable(xavi_init(shape=[128,2]),name='fc3_w')\n",
    "fc3_b = tf.Variable(tf.zeros(shape=[2]),name='fc3_b')\n",
    "logit = tf.nn.bias_add( tf.matmul(fc2,fc3_w) , fc3_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost function , accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=true_out,logits=logit)\n",
    "total_loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(total_loss)\n",
    "\n",
    "#train_op = tf.train.AdamOptimizer(0.001).minimize(total_loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logit, 1),tf.argmax(true_out, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "CHECKPOINT_PATH = './ckpt/vgg_xray.ckpt'\n",
    "CHECKPOINT_FILE = '%s.meta' % CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#batch_size = 500\n",
    "batch_size = 50\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(CHECKPOINT_FILE):\n",
    "        saver.restore(sess, CHECKPOINT_PATH)\n",
    "        print('Restoring values from %s...' % CHECKPOINT_PATH)\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "\n",
    "    for epoch in range(350):\n",
    "        \n",
    "        epoch_loss =0.0\n",
    "\n",
    "        for start in range(0,len(train_image),batch_size):\n",
    "\n",
    "            end = min( start+batch_size ,len(train_image))\n",
    "            image_iter = train_image[start:end]\n",
    "            label_iter = train_label[start:end]\n",
    "\n",
    "            _,train_loss = sess.run([train_op,total_loss], feed_dict={images: image_iter, true_out:label_iter})\n",
    "            epoch_loss += train_loss\n",
    "            if end == len(train_image) :\n",
    "                #loss,accr,train_logit = sess.run([total_loss,accuracy,logit], feed_dict={images: image_iter,kprob:0.8,true_out:label_iter})\n",
    "                #print(epoch,epoch_loss,accr)\n",
    "                print(epoch,epoch_loss)\n",
    "\n",
    "                \n",
    "        \n",
    "        tp= fn= fp= tn= vloss = 0.0\n",
    "        \n",
    "        for start in range(0,len(valid_image),batch_size):\n",
    "\n",
    "            end = min( start+batch_size ,len(valid_image))\n",
    "\n",
    "            image_iter = valid_image[start:end]\n",
    "            label_iter = valid_label[start:end]\n",
    "            \n",
    "            val_loss,val_logit = sess.run([total_loss,logit], feed_dict={images: image_iter, true_out:label_iter,kprob:1.0,train_mode:False})\n",
    "            \n",
    "            \n",
    "            \n",
    "            y_true = np.argmax(label_iter,axis=1)\n",
    "            y_pred = np.argmax(val_logit,axis=1) \n",
    "\n",
    "            true_index =np.where(y_true == 1)\n",
    "            flase_index = np.where(y_true == 0)\n",
    "\n",
    "\n",
    "            tp += np.sum(y_pred[true_index])\n",
    "            fn += np.sum(y_pred[true_index] == 0)\n",
    "            fp += np.sum(y_pred[flase_index])\n",
    "            tn += np.sum(y_pred[flase_index] == 0)\n",
    "            \n",
    "            vloss += val_loss\n",
    "\n",
    "        recall = tp/(tp+fn+1e-7)\n",
    "        precision = tp/(tp+fp+1e-7)\n",
    "        f1score = 2*(recall*precision)/(recall+precision+1e-7)\n",
    "\n",
    "\n",
    "        print('VALID ----- EPOCH : {:.3f}, LOSS : {:.3f}'.format(epoch,vloss))\n",
    "        print('RECALL :{:.3f} , PRECISION : {:.3f} , F1 SCORE :{:.3f}'.format(recall, precision, f1score ))\n",
    "        print('TP={}, TN={}, FP={}, FN={}'.format(tp,tn,fp,fn))\n",
    "        \n",
    "    #saver.save(sess, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = glob.glob('./data/test/*.png')\n",
    "test_image = []\n",
    "for f in X_test:\n",
    "    img = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n",
    "    m,s = cv2.meanStdDev(img)\n",
    "    std_img = (img- m)/(1.e-6 + s)\n",
    "    \n",
    "    test_image.append(std_img.reshape((64,64,1)))\n",
    "    \n",
    "print('test set' , np.shape(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(CHECKPOINT_FILE):\n",
    "        saver.restore(sess, CHECKPOINT_PATH)\n",
    "        print('Restoring values from %s...' % CHECKPOINT_PATH)\n",
    "               \n",
    "        \n",
    "        logit_list = []\n",
    "        \n",
    "        for start in range(0,len(test_image),batch_size):\n",
    "\n",
    "            end = start + batch_size  if start + batch_size  < len(test_image)  else  len(test_image)\n",
    "\n",
    "            image_iter = test_image[start:end]        \n",
    "            test_logit = sess.run([logit], feed_dict={images: image_iter,kprob:1.0,train_mode:False})\n",
    "            logit_list.append(test_logit)\n",
    "            \n",
    "        print('----done---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.row_stack([ i[0] for i in logit_list])\n",
    "pred_label = np.argmax(pred_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {'path':X_test,'label':pred_label}\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorgpu",
   "language": "python",
   "name": "tensorgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
